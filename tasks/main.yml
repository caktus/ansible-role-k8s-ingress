---
# Sanity check so we don't accidentally remove all access to the cluster on AWS:
- when: (k8s_cluster_type == "aws") and (k8s_iam_users|length == 0)
  fail:
    msg: "Set k8s_iam_users to a list of IAM usernames who should have access to manage the cluster"

- when: (k8s_cluster_type != "aws") and k8s_ci_create_user
  fail:
    msg: "Setting k8s_ci_create_user is only supported on AWS"

- name: Remove cert-manager
  k8s:
    context: "{{ k8s_context|mandatory }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    wait: yes
    definition: "{{ lookup('template', 'cert-manager.yaml') }}"
    state: absent
  when: k8s_purge_cert_manager

- name: Remove ingress-nginx
  k8s:
    context: "{{ k8s_context|mandatory }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    wait: yes
    definition: "{{ lookup('template', 'ingress-nginx/mandatory.yaml') }}"
    state: absent
  when: k8s_purge_ingress_controller

- name: Create ingress-nginx namespace
  community.kubernetes.k8s:
    context: "{{ k8s_context|mandatory }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    name: "{{ k8s_ingress_nginx_namespace }}"
    api_version: v1
    kind: Namespace
    state: present
  when: k8s_install_ingress_controller

- name: Create cert-manager namespace
  community.kubernetes.k8s:
    context: "{{ k8s_context|mandatory }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    name: "{{ k8s_cert_manager_namespace }}"
    api_version: v1
    kind: Namespace
    state: present
  when: k8s_install_cert_manager

- name: Add ingress-nginx Helm chart
  community.kubernetes.helm:
    context: "{{ k8s_context|mandatory }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    chart_repo_url: "https://kubernetes.github.io/ingress-nginx"
    chart_ref: ingress-nginx
    # https://github.com/kubernetes/ingress-nginx/tags
    chart_version: "{{ k8s_ingress_nginx_chart_version }}"
    release_name: ingress-nginx
    release_namespace: "{{ k8s_ingress_nginx_namespace }}"
    release_values: "{{ lookup('template', 'ingress-nginx/chart-values-%s.yaml' % k8s_cluster_type) | from_yaml }}"
    wait: yes
  when: k8s_install_ingress_controller

- name: Add cert-manager Helm chart
  community.kubernetes.helm:
    context: "{{ k8s_context|mandatory }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    chart_repo_url: "https://charts.jetstack.io"
    chart_ref: cert-manager
    chart_version: "{{ k8s_cert_manager_chart_version }}"
    release_name: cert-manager
    release_namespace: "{{ k8s_cert_manager_namespace }}"
    release_values:
      installCRDs: "true"
    wait: yes
  when: k8s_install_cert_manager

- name: Deploy Let's Encrypt issuers
  k8s:
    context: "{{ k8s_context|mandatory }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    wait: yes
    # cert-manager's custom resource definitions don't validate,
    # so this is disabled for now.
    # validate:
    #   fail_on_error: yes
    #   strict: yes
    definition: "{{ lookup('template', 'letsencrypt/issuers.yaml') }}"
  when: k8s_install_cert_manager

- name: Grant access to IAM users if aws
  when: k8s_cluster_type == "aws"
  k8s:
    definition: "{{ lookup('template', 'aws/cluster-auth.yaml') }}"
    context: "{{ k8s_context }}"
    kubeconfig: "{{ k8s_kubeconfig }}"
    state: present
    wait: yes
    validate:
      fail_on_error: yes
      strict: yes

- name: Create CI user
  iam:
    name: "{{ k8s_ci_username }}"
    state: present
    iam_type: user
    profile: "{{ k8s_ci_aws_profile }}"
  when: k8s_ci_create_user
  register: ci_user

- debug:
    msg: |
      IAM user {{ k8s_ci_username }} has been created.
      Create an access_key in the AWS IAM console and store them in your CI's environment variables.
  when: ci_user is changed

- name: Attach inline policy to user
  iam_policy:
    iam_type: user
    iam_name: "{{ k8s_ci_username }}"
    policy_name: "ECRPush"
    state: present
    policy_json: "{{ lookup( 'template', 'aws/ECRPush.json.j2') }}"
    profile: "{{ k8s_ci_aws_profile }}"
  when: k8s_ci_create_user
